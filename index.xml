<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Student Portfolio on Sampad Kumar Kar</title>
    <link>https://sampadk04.github.io/</link>
    <description>Recent content in Student Portfolio on Sampad Kumar Kar</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://sampadk04.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>App: My ToDo WebApp</title>
      <link>https://sampadk04.github.io/post/project-7/</link>
      <pubDate>Mon, 06 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://sampadk04.github.io/post/project-7/</guid>
      <description>Test the webapp @https://webappmytodo.herokuapp.com Snaps  How to Use? Add the title of your task in the title box. Add the description of your task in the description box. Click on the Add Task button to add the task to you ToDo List.  View your task in the ToDo List.  Update/Delete your task by clicking on the relevant buttons in the ToDo List.    Tools Used  Python Flask, Flask SQLAlchemy HTML, CSS(Bootstrap) SQLite Heroku(Deployment)   Link to GitHub Repository</description>
    </item>
    
    <item>
      <title>Project: Fast Text based Clustering</title>
      <link>https://sampadk04.github.io/post/project-6/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://sampadk04.github.io/post/project-6/</guid>
      <description>(This project primarily uses the sklearn library for implementation)
 We look at various techniques of clustering while using the Jaccard Similarity as a measure of closeness. We first used the built-in KMeans by processing the documents as binary vectors with dimension equal to the no. of words in the vocabulary. We used this for KOS and NIPS Since, the above techniques turned out to be too slow or unreliable (the MiniBatchKMeans) for Enron, we created 2 algorithms from scratch to tackle while considering the Jaccard Distance.</description>
    </item>
    
    <item>
      <title>Project: Image Classification with Perceptrons</title>
      <link>https://sampadk04.github.io/post/project-5/</link>
      <pubDate>Sun, 06 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://sampadk04.github.io/post/project-5/</guid>
      <description>(This project primarily uses the sklearn library for implementation)
 In this project, I use the Perceptron Classifier to clasifiy the images in MNIST_784 dataset. I do this in two steps:  Firstly, I build a binary classifier to classify digits labeled &amp;lsquo;0&amp;rsquo; and &amp;lsquo;not 0&amp;rsquo;. Secondly, I combine this classifer 10 times to classify all 10 digits in a &amp;ldquo;One vs All&amp;rdquo; fashion to build a multi-class classifier.   I also test the models with and without cross-validations.</description>
    </item>
    
    <item>
      <title>Project: Logistic Regression: Implementation and Visualization from scratch</title>
      <link>https://sampadk04.github.io/post/project-2/</link>
      <pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://sampadk04.github.io/post/project-2/</guid>
      <description>I implement binary logistic regression from scratch and employ it for solving a couple of classification problems. In one of the setting, we have linearly separable classes and the other one has non-linear decision boundary between classes. The first problem could be addressed with basic logistic regression classifier, while the second requires additional step of polynomial transformation before using Logistic Regression. I also analyze the learning curves of individual models of iterative optimization, i.</description>
    </item>
    
    <item>
      <title>Project: Perceptrons from scratch</title>
      <link>https://sampadk04.github.io/post/project-4/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://sampadk04.github.io/post/project-4/</guid>
      <description>I implement the perceptron classifier from scratch. I look at 3 different cases and analyze those using our model:  First, for linearly separable data. Second, for linearly inseparable data (with approximate linear boundary fit). Third, for non-linearly separable data.   I also look at the learning curves and analyze the loss. Finally, I visualize the decision boundary obtained by the model in each instance.   Link to GitHub Repository</description>
    </item>
    
    <item>
      <title>Project: Least Square Classifier from scratch</title>
      <link>https://sampadk04.github.io/post/project-3/</link>
      <pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://sampadk04.github.io/post/project-3/</guid>
      <description>I implement Least Square Classifier (LSC) from scratch. I also implement label encoder to transform the output labels into usable form (for both binary and multi-class) classification setup. I have addressed four types of problems:  first with binary linearly separable data without outliers second with binary separable data with outliers third with multiclass separable data (in our case we looked at separating into three classes) fourth with non linearly separable data   The first three problems (binary and multiclass) could be addressed with basic LSC, while the fourth required additional step of polynomial transformation before employing LSC.</description>
    </item>
    
    <item>
      <title>Project: Exploring Model Selection via Linear Regression</title>
      <link>https://sampadk04.github.io/post/project-1/</link>
      <pubDate>Sat, 29 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://sampadk04.github.io/post/project-1/</guid>
      <description>I built different linear regression models using California Housing Data fetched from sklearn.datasets:  Linear regression (with normal equation and iterative optimization (SGD)), Polynomial Regression. Regularizarized regression models - Ridge, Lasso, Elastinet.   I tuned polynomial degree, regularization rate and learning rate with hyper-parameter tuning and cross-validation. I also employed Grid Search and Randomized Search via GridSearchCV and RandomizedSearchCV to tune multiple hyperparameters. Lastly, I compared different models in terms of their parameter vectors and mean absolute error on train, dev (validation) and test sets, and analyzed their evaluation score on the test dataset.</description>
    </item>
    
    <item>
      <title>Report: A Comparison of Regularization Techniques in DNN</title>
      <link>https://sampadk04.github.io/post/project-0/</link>
      <pubDate>Tue, 11 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://sampadk04.github.io/post/project-0/</guid>
      <description>Reading Project Report Artificial Neural Networks (ANN) have garnered significant attention from researchers and engineers because of its ability to solve many complex problems with reasonable ability. If enough data are provided during the training process, ANNs are capable of achieving good performance results. However, if training data are not enough, the predefined neural network model suffers from overfitting and underfitting problems. To solve these problems, several regularization techniques have been devised and widely applied to ANNs.</description>
    </item>
    
  </channel>
</rss>
